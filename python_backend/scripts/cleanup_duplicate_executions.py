"""
æ¸…ç† JSONL åŸ·è¡Œè¨˜éŒ„ä¸­çš„é‡è¤‡è¨˜éŒ„

ç”¨é€”ï¼šç§»é™¤å› ç‚ºèˆŠç‰ˆæœ¬ bug ç”¢ç”Ÿçš„é‡è¤‡è¨˜éŒ„
è¦å‰‡ï¼šå°æ–¼æ¯å€‹ pairIdï¼Œå¦‚æœæœ‰å¤šç­†è¨˜éŒ„ï¼Œåªä¿ç•™æˆåŠŸè¨˜éŒ„ï¼›å¦‚æœæ²’æœ‰æˆåŠŸè¨˜éŒ„ï¼Œä¿ç•™æœ€å¾Œä¸€ç­†
"""

import os
import json
import time
from typing import Dict, List
from collections import defaultdict


def cleanup_jsonl_file(file_path: str, dry_run: bool = True) -> None:
    """
    æ¸…ç†æŒ‡å®šçš„ JSONL æ–‡ä»¶
    
    Args:
        file_path: JSONL æ–‡ä»¶è·¯å¾‘
        dry_run: å¦‚æœç‚º Trueï¼Œåªé¡¯ç¤ºæœƒåšä»€éº¼ï¼Œä¸å¯¦éš›ä¿®æ”¹æ–‡ä»¶
    """
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    print(f"ğŸ“‚ è™•ç†æ–‡ä»¶: {file_path}")
    print(f"ğŸ”§ æ¨¡å¼: {'æ¨¡æ“¬é‹è¡Œï¼ˆä¸æœƒä¿®æ”¹æ–‡ä»¶ï¼‰' if dry_run else 'å¯¦éš›æ¸…ç†'}")
    
    # è®€å–æ‰€æœ‰è¨˜éŒ„
    records: List[dict] = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                record = json.loads(line.strip())
                records.append(record)
            except json.JSONDecodeError as e:
                print(f"âš ï¸ ç¬¬ {line_num} è¡Œ JSON è§£æå¤±æ•—: {e}")
    
    print(f"ğŸ“Š è®€å–è¨˜éŒ„ç¸½æ•¸: {len(records)}")
    
    # æŒ‰ pairId åˆ†çµ„
    grouped: Dict[str, List[dict]] = defaultdict(list)
    for record in records:
        pair_id = record.get('pairId', 'unknown')
        grouped[pair_id].append(record)
    
    print(f"ğŸ“¦ ä¸åŒçš„ pairId æ•¸é‡: {len(grouped)}")
    
    # éæ¿¾é‡è¤‡è¨˜éŒ„
    filtered_records: List[dict] = []
    duplicate_count = 0
    
    for pair_id, pair_records in grouped.items():
        if len(pair_records) == 1:
            # åªæœ‰ä¸€ç­†è¨˜éŒ„ï¼Œç›´æ¥ä¿ç•™
            filtered_records.append(pair_records[0])
        else:
            # æœ‰å¤šç­†è¨˜éŒ„ï¼Œéœ€è¦éæ¿¾
            print(f"\nğŸ” pairId: {pair_id} æœ‰ {len(pair_records)} ç­†è¨˜éŒ„")
            
            # å„ªå…ˆæ‰¾æˆåŠŸè¨˜éŒ„
            success_records = [r for r in pair_records if r.get('status') == 'success']
            
            if success_records:
                # å¦‚æœæœ‰æˆåŠŸè¨˜éŒ„ï¼Œé¸æ“‡ç¬¬ä¸€ç­†æˆåŠŸè¨˜éŒ„
                selected = success_records[0]
                print(f"  âœ… é¸æ“‡æˆåŠŸè¨˜éŒ„ (ts={selected.get('ts')})")
                filtered_records.append(selected)
                duplicate_count += len(pair_records) - 1
                
                # é¡¯ç¤ºè¢«ç§»é™¤çš„è¨˜éŒ„
                for r in pair_records:
                    if r != selected:
                        print(f"  ğŸ—‘ï¸ ç§»é™¤è¨˜éŒ„: status={r.get('status')}, ts={r.get('ts')}")
            else:
                # æ²’æœ‰æˆåŠŸè¨˜éŒ„ï¼Œé¸æ“‡æœ€å¾Œä¸€ç­†ï¼ˆæŒ‰æ™‚é–“æˆ³ï¼‰
                sorted_records = sorted(pair_records, key=lambda r: r.get('ts', 0), reverse=True)
                selected = sorted_records[0]
                print(f"  ğŸ“ é¸æ“‡æœ€å¾Œè¨˜éŒ„: status={selected.get('status')}, ts={selected.get('ts')}")
                filtered_records.append(selected)
                duplicate_count += len(pair_records) - 1
                
                # é¡¯ç¤ºè¢«ç§»é™¤çš„è¨˜éŒ„
                for r in sorted_records[1:]:
                    print(f"  ğŸ—‘ï¸ ç§»é™¤è¨˜éŒ„: status={r.get('status')}, ts={r.get('ts')}")
    
    print(f"\nğŸ“Š æ¸…ç†çµæœ:")
    print(f"  åŸå§‹è¨˜éŒ„æ•¸: {len(records)}")
    print(f"  æ¸…ç†å¾Œè¨˜éŒ„æ•¸: {len(filtered_records)}")
    print(f"  ç§»é™¤é‡è¤‡è¨˜éŒ„: {duplicate_count}")
    
    if not dry_run:
        # å‚™ä»½åŸæ–‡ä»¶
        backup_path = f"{file_path}.backup.{int(time.time())}"
        print(f"\nğŸ’¾ å‚™ä»½åŸæ–‡ä»¶åˆ°: {backup_path}")
        with open(backup_path, 'w', encoding='utf-8') as f:
            with open(file_path, 'r', encoding='utf-8') as original:
                f.write(original.read())
        
        # å¯«å…¥æ¸…ç†å¾Œçš„è¨˜éŒ„
        print(f"âœï¸ å¯«å…¥æ¸…ç†å¾Œçš„è¨˜éŒ„...")
        with open(file_path, 'w', encoding='utf-8') as f:
            for record in filtered_records:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
        
        print(f"âœ… æ¸…ç†å®Œæˆï¼")
    else:
        print(f"\nğŸ’¡ æç¤º: ä½¿ç”¨ --execute åƒæ•¸åŸ·è¡Œå¯¦éš›æ¸…ç†")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¸…ç† JSONL åŸ·è¡Œè¨˜éŒ„ä¸­çš„é‡è¤‡è¨˜éŒ„')
    parser.add_argument('--file', '-f', type=str, help='æŒ‡å®šè¦æ¸…ç†çš„ JSONL æ–‡ä»¶')
    parser.add_argument('--all', '-a', action='store_true', help='æ¸…ç†æ‰€æœ‰åŸ·è¡Œè¨˜éŒ„æ–‡ä»¶')
    parser.add_argument('--execute', '-e', action='store_true', help='å¯¦éš›åŸ·è¡Œæ¸…ç†ï¼ˆé»˜èªç‚ºæ¨¡æ“¬é‹è¡Œï¼‰')
    
    args = parser.parse_args()
    
    # ç¢ºå®šæ•¸æ“šç›®éŒ„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.path.abspath(os.path.join(script_dir, '../../..'))
    data_dir = os.path.join(base_dir, 'data', 'arbitrage')
    
    print("=" * 60)
    print("ğŸ§¹ JSONL åŸ·è¡Œè¨˜éŒ„æ¸…ç†å·¥å…·")
    print("=" * 60)
    print(f"ğŸ“ æ•¸æ“šç›®éŒ„: {data_dir}")
    print()
    
    if args.file:
        # æ¸…ç†æŒ‡å®šæ–‡ä»¶
        cleanup_jsonl_file(args.file, dry_run=not args.execute)
    elif args.all:
        # æ¸…ç†æ‰€æœ‰åŸ·è¡Œè¨˜éŒ„æ–‡ä»¶
        if not os.path.exists(data_dir):
            print(f"âŒ æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
            return
        
        jsonl_files = [f for f in os.listdir(data_dir) if f.startswith('executions_') and f.endswith('.jsonl')]
        
        if not jsonl_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°åŸ·è¡Œè¨˜éŒ„æ–‡ä»¶")
            return
        
        print(f"ğŸ“‚ æ‰¾åˆ° {len(jsonl_files)} å€‹åŸ·è¡Œè¨˜éŒ„æ–‡ä»¶:")
        for f in jsonl_files:
            print(f"  - {f}")
        print()
        
        for filename in jsonl_files:
            file_path = os.path.join(data_dir, filename)
            cleanup_jsonl_file(file_path, dry_run=not args.execute)
            print()
    else:
        # é»˜èªæ¸…ç†ç•¶æ—¥æ–‡ä»¶
        day_str = time.strftime('%Y%m%d')
        file_path = os.path.join(data_dir, f'executions_{day_str}.jsonl')
        
        if os.path.exists(file_path):
            cleanup_jsonl_file(file_path, dry_run=not args.execute)
        else:
            print(f"âŒ ç•¶æ—¥åŸ·è¡Œè¨˜éŒ„æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ --all åƒæ•¸æ¸…ç†æ‰€æœ‰æ–‡ä»¶")


if __name__ == '__main__':
    main()

